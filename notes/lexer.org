#+SETUPFILE: setup.org

* DRAFT lexer
  :LOGBOOK:
  - Note taken on [2020-12-04 五 08:46] \\
    compiler section describe lex part in whole chain.
  :END:

编程语言从文件角度来看，只是单纯的文本文件，由字符组成。

至于作为编程语言来由机器理解并执行，需要经过一系列的过程。
不同组件有明确的分工，不同的组件有不同的输入和输出，组成上下游关系。

词法分析 lex 就是第一步。

#+CAPTION: char stream to token stream

lexer 将源代码转换为 token，将字符流转化为 token 流，作为 parser 的输入。

** token

简单的说，token 就是多个有序字符组成的序列。

lexer 按照一定的规则，在字符流中寻找并将匹配为 token。

lua 中定义了如下 token

#+INCLUDE: ../lua-5.1.5/src/llex.h src c :lines "14-34"

#+INCLUDE: ../lua-5.1.5/src/llex.c src c :lines "36-46"

#+INCLUDE: ../lua-5.1.5/src/llex.h src c :lines "43-53"

所有匹配的 token 都用 Token 来表示，其中
- token，即上面的 enum 值，表示 token 的类型
  - 单字符 char token，比如 =( , . ; [= 之类，这也是 FIRST_RESERVED 从 257 开始的原因
- Seminfo，用于存储 =<number> <name> <string>= 的实际内容
  - r =number= 数字值
  - ts =name= 字面字符串， =string= 的字符串内容

#+CAPTION: some example

** process

#+CAPTION: 匹配过程，with eg code example

转化的过程，简单描述，就是从输入流的头部开始匹配，返回匹配的 token

一般而言，这是一个相对枯燥又考验耐心的工作。
通常用 regex 来描述定义的 token，再将其转化为代码的形式。

一般一个语言的开发初期，都会使用 lexer generator 比如 flex 这样的工具，
自动将 regex 生成相应匹配的代码，方便快速迭代，sketch，进行语言的开发。

但到了后期发展相对成熟的时候，为了效率，就会将这部分代码重写，py ry 如此，lua 也是如此。

看似简单，实际有底层严谨的数学支撑，regex NFA DFA 再到代码呈现。

这一段是相对符合程序员直觉的，有更多专业的书讲解的更清楚，具体关于模式匹配的内容就不再赘述。

TODO ref materials

至于如何匹配，都在 llex 中实现

提到几个值得关注的点，

- 只有 name 匹配过程，而没有关键字的匹配过程
  如果 name 和关键字相同，就认为是关键字，这也是为什么 name 决不能是关键字的原因
  直接导致语法错误

#+begin_src lua
local end = 1
#+end_src


实际实现中，并非所有都是借助 regex 的过程来实现的

对数字的检测，正则检测并不完整

#+INCLUDE: ../lua-5.1.5/src/llex.c src c :lines "193-208"

比如匹配数字的 read numeral 方法，从 llex 到这里

TODO 如果直接从 regex 的角度来看，模式是 =(.\d | \d)(. | \d)*[Ee[+-]]=??

明显这不是正确的数字匹配模式， =10.3.3.3= 就可以匹配，但是并不是数字。
关于这一步，lua 使用了 c lib 中的 strtod 来尝试进行转换，如果发生错误，则不是数字

有些取巧的做法



lua 中有一种长字符串表示，但是同样存在一种变体， ~=~ 的数量要完全相同，才是正确的匹配
长字符串的规则，同样可以扩充到长注释。

这一点在阅读代码时要注意。

#+begin_src lua
local long_str = [[
this is a long string.
]]

local another_str = [==[
another long string.
]==]

--[[
comment this line
]]

--[====[
comment this line
]====]
#+end_src


** inside

*** lexstate
    :LOGBOOK:
    - Note taken on [2020-12-04 五 11:13] \\
      ls 和 fs 及 f 的全局安排，图解
    :END:

在 lex 的过程中，需要有相应的数据来记录匹配的过程和结果，这个数据结构就是 lexstate

这个结构在整个 front 过程是会经常出现，核心结构之一，所以清楚其概念是非常大的收益

#+INCLUDE: ../lua-5.1.5/src/llex.h src c :lines "55-68"

- current 当前 token 之后紧跟的字符
- linenumber 行号
- lastline 上一个 token 的行号
- t 当前 token
- lookahead 前瞻的 token
- fs pointer
- L pointer
- z input stream pointer
- buff pointer
- source name
- decpoint locale decimal point

#+CAPTION: lex state inside picture, with line number

核心数据形式定义了，lex 的其余部分就是对数据的操作

zio -> buffer -> token -->> lookahead

它和 nil 一样，是一个 singleton 结构，只出现在 front 阶段，做好本职工作

*** =luaX_init=

先提一下 init 过程，和之前 tstring 连接起来

前面提到，其中有 reserved 字段，就在 lex 这里发挥作用。

init 过程，对所有 token 类型新建了 tstring（存在于全局表），
gc fix
并将 reverved 设置为从 1 开始的偏移量，标识类型，再后面的
name -> keyword 过程中发挥作用。

从这里也看出，ts 不只用于对外，也在内部运作中产生重要作用

*** =luaX_next=

使用 llex 过程，得到 token 内容，并返回 token 类型

#+CAPTION: zio -> mbuffer -> token picture

语法分析中，有时候需要进行前瞻 lookahead，来进行下一步的运作

lookahead 同样使用 llex，不过存储在 lookahead 中而不是 t

在 next 中可以发现，如果存在 lookahead，则顺利交接


同时也要注意，lookahead 内部的实现，说明无法进行连续的 lookahead 调用，这样会在 next 中丢失


** lexer program

根据对 lex 模块的理解，可以利用其中的方法，做一个简单的 lexer 分析器。

首先，它不是一个独立的程序，而是一段 gdb 脚本，可以在 lua 的运行时，进行 inspect。
我们在 next 方法加上断点，每次触发的时候，就输出相应的 token

就可以完美的分析出词法过程。

默认编译 lua 有
- lua 解释器
- luac 编译器

这里并不需要相应代码的执行结果，所以使用 luac 就足够

#+begin_src bash
make -s lex
#+end_src


这种做法也有弱点，因为其包含了整个编译过程，所以是无法正常分析语法错误的代码的
只能分析正常使用的代码。

TODO 或许可以自己主动调用 next，这样就可以避免上面的约束？


** practice

llex.h.c rest

对 io 部分的抽象，建议阅读 zio.h.c

- zio input stream
- mbuffer mini buffer


*** more theory

with materials

regex
DFA
NFA

flex 原理

将 regex 表示为代码逻辑


